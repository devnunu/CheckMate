# .github/scripts/universal_line_analyzer.py
import os
import re
import openai
from github import Github
from typing import List, Dict
from universal_code_analyzer import UniversalCodeAnalyzer

class UniversalLineAnalyzer:
    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])
        self.github_client = Github(os.environ['GITHUB_TOKEN'])
        self.repo_name = os.environ['REPO_NAME']
        self.pr_number = int(os.environ['PR_NUMBER'])

        self.repo = self.github_client.get_repo(self.repo_name)
        self.pr = self.repo.get_pull(self.pr_number)

        # ë²”ìš© ì½”ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.universal_analyzer = UniversalCodeAnalyzer(self.repo, self.pr)

    def parse_diff_for_changed_lines(self, patch: str) -> Dict[int, int]:
        """diff patchë¥¼ íŒŒì‹±í•˜ì—¬ ì‹¤ì œ ë³€ê²½ëœ ë¼ì¸ ë²ˆí˜¸ì™€ diff position ë§¤í•‘"""
        if not patch:
            return {}

        line_mapping = {}  # {ì‹¤ì œ_íŒŒì¼_ë¼ì¸: diff_position}
        current_file_line = 0
        diff_position = 0

        lines = patch.split('\n')

        for line in lines:
            if line.startswith('@@'):
                # @@ -old_start,old_count +new_start,new_count @@ í˜•ì‹ íŒŒì‹±
                match = re.search(r'\+(\d+)', line)
                if match:
                    current_file_line = int(match.group(1)) - 1
            elif line.startswith('+'):
                # ì¶”ê°€ëœ ë¼ì¸
                current_file_line += 1
                line_mapping[current_file_line] = diff_position
            elif line.startswith(' '):
                # ë³€ê²½ë˜ì§€ ì•Šì€ ë¼ì¸ (ì»¨í…ìŠ¤íŠ¸)
                current_file_line += 1
                line_mapping[current_file_line] = diff_position
            # '-'ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ì€ ì‚­ì œëœ ë¼ì¸ì´ë¯€ë¡œ current_file_line ì¦ê°€í•˜ì§€ ì•ŠìŒ

            diff_position += 1

        return line_mapping

    def get_changed_lines_only(self, file_path: str, patch: str) -> List[int]:
        """ì‹¤ì œë¡œ ë³€ê²½ëœ ë¼ì¸ ë²ˆí˜¸ë§Œ ì¶”ì¶œ (+ ë¼ì¸)"""
        if not patch:
            return []

        changed_lines = []
        current_file_line = 0

        lines = patch.split('\n')

        for line in lines:
            if line.startswith('@@'):
                # @@ -old_start,old_count +new_start,new_count @@ í˜•ì‹ íŒŒì‹±
                match = re.search(r'\+(\d+)', line)
                if match:
                    current_file_line = int(match.group(1)) - 1
            elif line.startswith('+'):
                # ì¶”ê°€ëœ ë¼ì¸ë§Œ ë¶„ì„ ëŒ€ìƒ
                current_file_line += 1
                changed_lines.append(current_file_line)
            elif line.startswith(' '):
                # ì»¨í…ìŠ¤íŠ¸ ë¼ì¸
                current_file_line += 1
            # '-' ë¼ì¸ì€ ë¬´ì‹œ

        return changed_lines

    def analyze_file_for_issues(self, file_path: str, file_content: str, patch: str) -> List[Dict]:
        """íŒŒì¼ ë¶„ì„ - ë³€ê²½ëœ ë¼ì¸ë§Œ ëŒ€ìƒìœ¼ë¡œ"""

        language = self.universal_analyzer.detect_language(file_path)
        if not language:
            print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
            return []

        # ì‹¤ì œ ë³€ê²½ëœ ë¼ì¸ë§Œ ê°€ì ¸ì˜¤ê¸°
        changed_lines = self.get_changed_lines_only(file_path, patch)
        if not changed_lines:
            print(f"  âš ï¸ ë³€ê²½ëœ ë¼ì¸ì´ ì—†ìŒ: {file_path}")
            return []

        print(f"  ğŸ“ {file_path}: {len(changed_lines)}ê°œ ë¼ì¸ ë³€ê²½ë¨")

        # ë³€ê²½ëœ ë¼ì¸ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        file_lines = file_content.split('\n')
        analysis_chunks = []

        for line_num in changed_lines:
            # ë³€ê²½ëœ ë¼ì¸ ì£¼ë³€ Â±3ë¼ì¸ ì»¨í…ìŠ¤íŠ¸ í¬í•¨
            start_line = max(1, line_num - 3)
            end_line = min(len(file_lines), line_num + 3)

            chunk_lines = []
            for i in range(start_line - 1, end_line):  # 0-based index
                if i < len(file_lines):
                    prefix = ">>>" if (i + 1) == line_num else "   "  # ë³€ê²½ ë¼ì¸ í‘œì‹œ
                    chunk_lines.append(f"{prefix} {i + 1:3d}: {file_lines[i]}")

            analysis_chunks.append({
                'target_line': line_num,
                'context': '\n'.join(chunk_lines)
            })

        # AI ë¶„ì„ ì‹¤í–‰
        all_issues = []
        for chunk in analysis_chunks:
            issues = self.analyze_chunk_with_ai(file_path, chunk, language)
            all_issues.extend(issues)

        return all_issues

    def analyze_chunk_with_ai(self, file_path: str, chunk: Dict, language: str) -> List[Dict]:
        """AI ê¸°ë°˜ ì½”ë“œ ì²­í¬ ë¶„ì„"""

        analysis_prompt = f"""
íŒŒì¼: {file_path} (ì–¸ì–´: {language})
ë³€ê²½ëœ ë¼ì¸: {chunk['target_line']}

ì½”ë“œ ì»¨í…ìŠ¤íŠ¸:
```
{chunk['context']}
```

>>> í‘œì‹œëœ ë¼ì¸({chunk['target_line']})ì´ ìƒˆë¡œ ì¶”ê°€ë˜ê±°ë‚˜ ìˆ˜ì •ëœ ì½”ë“œì…ë‹ˆë‹¤.

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. **ë„¤ì´ë°**: ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª…ì´ ëª…í™•í•˜ê³  ì¼ê´€ì ì¸ê°€?
2. **ë¡œì§ ì˜¤ë¥˜**: ì¡°ê±´ë¬¸, ë°˜ë³µë¬¸ì—ì„œ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ë™ì‘ ê°€ëŠ¥ì„±
3. **ë„ ì•ˆì „ì„±**: null ì²´í¬ ëˆ„ë½, ì˜µì…”ë„ ì²˜ë¦¬ ë¯¸í¡
4. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ë¦¬ì†ŒìŠ¤ í•´ì œ, ê°•í•œ ì°¸ì¡° ìˆœí™˜
5. **ì„±ëŠ¥**: ë¹„íš¨ìœ¨ì ì¸ ì—°ì‚°, ë¶ˆí•„ìš”í•œ ê°ì²´ ìƒì„±
6. **ì—ëŸ¬ ì²˜ë¦¬**: ì˜ˆì™¸ ìƒí™© ëŒ€ì‘ ë¶€ì¡±

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì—†ì´):
[
  {{
    "line": {chunk['target_line']},
    "priority": "P2"|"P3",
    "category": "ë„¤ì´ë°|ë¡œì§|ë„ì•ˆì „ì„±|ë©”ëª¨ë¦¬|ì„±ëŠ¥|ì—ëŸ¬ì²˜ë¦¬",
    "message": "ë¬¸ì œì ì„ 50ì ì´ë‚´ë¡œ",
    "suggestion": "ê°œì„  ë°©ì•ˆì„ í•œ ì¤„ë¡œ"
  }}
]

ë¬¸ì œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ []ì„ ë°˜í™˜í•˜ì„¸ìš”.
ì‹¤ì œ ë¬¸ì œê°€ ìˆì„ ë•Œë§Œ í¬í•¨í•˜ê³ , ë³€ê²½ëœ ë¼ì¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì´ìŠˆë§Œ ì§€ì í•˜ì„¸ìš”.
"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"ìˆœìˆ˜ JSONë§Œ ì‘ë‹µí•˜ëŠ” {language} ì½”ë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³€ê²½ëœ ë¼ì¸ë§Œ ì§‘ì¤‘ ë¶„ì„í•˜ì„¸ìš”."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=800,
                temperature=0.1
            )

            response_text = response.choices[0].message.content.strip()
            response_text = self.clean_json_response(response_text)

            import json
            try:
                issues = json.loads(response_text)
                return issues if isinstance(issues, list) else []
            except json.JSONDecodeError as e:
                print(f"AI ë¶„ì„ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:200]}...")
                return []

        except Exception as e:
            print(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def clean_json_response(self, response_text: str) -> str:
        """AI ì‘ë‹µì—ì„œ ìˆœìˆ˜ JSONë§Œ ì¶”ì¶œ"""
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if "```json" in response_text:
            start = response_text.find("```json") + 7
            end = response_text.find("```", start)
            if end != -1:
                response_text = response_text[start:end].strip()
            else:
                response_text = response_text[start:].strip()
        elif "```" in response_text:
            start = response_text.find("```") + 3
            end = response_text.find("```", start)
            if end != -1:
                response_text = response_text[start:end].strip()

        # ì•ë’¤ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        response_text = response_text.strip()

        # JSON ë°°ì—´ì´ ì‹œì‘í•˜ëŠ” ì§€ì  ì°¾ê¸°
        start_bracket = response_text.find('[')
        if start_bracket != -1:
            response_text = response_text[start_bracket:]

        # JSON ë°°ì—´ì´ ëë‚˜ëŠ” ì§€ì  ì°¾ê¸° (ë§ˆì§€ë§‰ ]ê¹Œì§€)
        end_bracket = response_text.rfind(']')
        if end_bracket != -1:
            response_text = response_text[:end_bracket + 1]

        return response_text

    def create_review_comments(self, all_issues: Dict[str, List[Dict]]):
        """GitHub Review APIë¡œ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ ìƒì„± (ì •í™•í•œ ë¼ì¸ ë§¤í•‘)"""

        if not any(all_issues.values()):
            print("ë°œê²¬ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        comments = []
        total_comments = 0

        for file_path, issues in all_issues.items():
            if not issues:
                continue

            # í•´ë‹¹ íŒŒì¼ì˜ diff ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            pr_file = None
            for file in self.pr.get_files():
                if file.filename == file_path:
                    pr_file = file
                    break

            if not pr_file or not pr_file.patch:
                print(f"âš ï¸ {file_path}: diff ì •ë³´ ì—†ìŒ, ì½”ë©˜íŠ¸ ê±´ë„ˆë›°ê¸°")
                continue

            # diff ë¼ì¸ ë§¤í•‘ ìƒì„±
            line_mapping = self.parse_diff_for_changed_lines(pr_file.patch)
            language = self.universal_analyzer.detect_language(file_path)

            for issue in issues:
                file_line = issue['line']

                # í•´ë‹¹ ë¼ì¸ì´ ì‹¤ì œë¡œ ë³€ê²½ëœ ë¼ì¸ì¸ì§€ í™•ì¸
                if file_line not in line_mapping:
                    print(f"âš ï¸ {file_path}:{file_line} - ë³€ê²½ë˜ì§€ ì•Šì€ ë¼ì¸, ì½”ë©˜íŠ¸ ê±´ë„ˆë›°ê¸°")
                    continue

                diff_position = line_mapping[file_line]

                # ìš°ì„ ìˆœìœ„ë³„ ì´ëª¨ì§€
                priority_emoji = {'P2': 'ğŸŸ¡', 'P3': 'ğŸ”µ'}

                comment_body = f"{priority_emoji.get(issue['priority'], 'ğŸ“')} **[{issue['priority']}] AI ë¶„ì„**\n\n"
                comment_body += f"**{issue['category']}**: {issue['message']}\n"

                if issue.get('suggestion'):
                    comment_body += f"\n**ğŸ’¡ ê°œì„  ì œì•ˆ:**\n```{language}\n{issue['suggestion']}\n```"

                # GitHub Review API ì½”ë©˜íŠ¸ í˜•ì‹ (position ê¸°ë°˜)
                comments.append({
                    'path': file_path,
                    'body': comment_body,
                    'position': diff_position  # diff ë‚´ ìœ„ì¹˜ ì‚¬ìš©
                })
                total_comments += 1

        if not comments:
            print("âš ï¸ ìƒì„±í•  ìˆ˜ ìˆëŠ” ì½”ë©˜íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # GitHub Review ìƒì„±
        try:
            review = self.pr.create_review(
                event="COMMENT",
                comments=comments
            )

            print(f"âœ… ì´ {total_comments}ê°œ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ê°€ ì •í™•í•œ ìœ„ì¹˜ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {review.html_url}")

        except Exception as e:
            print(f"âŒ ë¼ì¸ë³„ ë¦¬ë·° ìƒì„± ì‹¤íŒ¨: {e}")
            print("ğŸ”§ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì¼ë°˜ ì½”ë©˜íŠ¸ ìƒì„±...")
            self.create_fallback_comment(all_issues)

    def create_fallback_comment(self, all_issues: Dict[str, List[Dict]]):
        """Review API ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì½”ë©˜íŠ¸ë¡œ ëŒ€ì²´"""
        comment_body = "ğŸ¤– **AI ì½”ë“œ ë¶„ì„ ê²°ê³¼**\n\n"

        for file_path, issues in all_issues.items():
            if issues:
                language = self.universal_analyzer.detect_language(file_path)
                comment_body += f"\n### ğŸ“ {file_path} ({language})\n"

                for issue in issues:
                    priority_emoji = {'P2': 'ğŸŸ¡', 'P3': 'ğŸ”µ'}
                    comment_body += f"- **Line {issue['line']}** {priority_emoji.get(issue['priority'], 'ğŸ“')} [{issue['priority']}] {issue['category']}: {issue['message']}\n"

        try:
            self.pr.create_issue_comment(comment_body)
            print("âœ… ëŒ€ì²´ ì½”ë©˜íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ëŒ€ì²´ ì½”ë©˜íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨: {e}")

    def run_universal_analysis(self):
        """ë²”ìš© ë¶„ì„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ” ë²”ìš© ì½”ë“œ í’ˆì§ˆ ê²€ìˆ˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        supported_extensions = self.universal_analyzer.get_supported_extensions()

        # PRì˜ ë³€ê²½ëœ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
        files = self.pr.get_files()
        all_issues = {}
        analyzed_count = 0
        skipped_count = 0

        for file in files:
            # ì‚­ì œëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°
            if file.status == 'removed':
                continue

            # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì í™•ì¸
            is_supported = any(file.filename.endswith(ext) for ext in supported_extensions)
            if not is_supported:
                skipped_count += 1
                continue

            print(f"ğŸ“ ë¶„ì„ ì¤‘: {file.filename}")
            analyzed_count += 1

            try:
                # í˜„ì¬ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                content = self.repo.get_contents(file.filename, ref=self.pr.head.sha)
                file_content = content.decoded_content.decode('utf-8')

                # ë³€ê²½ëœ ë¼ì¸ë§Œ ë¶„ì„
                issues = self.analyze_file_for_issues(
                    file.filename,
                    file_content,
                    file.patch or ""
                )

                if issues:
                    all_issues[file.filename] = issues
                    print(f"  âš ï¸ {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")
                else:
                    print(f"  âœ… ì´ìŠˆ ì—†ìŒ")

            except Exception as e:
                print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue

        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ë¶„ì„ ì™„ë£Œ: {analyzed_count}ê°œ íŒŒì¼ ë¶„ì„, {skipped_count}ê°œ íŒŒì¼ ê±´ë„ˆë›°ê¸°")

        # ë¼ì¸ë³„ ì½”ë©˜íŠ¸ ìƒì„±
        if all_issues:
            total_issues = sum(len(issues) for issues in all_issues.values())
            print(f"ğŸ“ˆ ì´ {total_issues}ê°œ ì´ìŠˆ ë°œê²¬")
            self.create_review_comments(all_issues)
        else:
            print("âœ… ëª¨ë“  ë¶„ì„ ëŒ€ìƒ íŒŒì¼ì´ í’ˆì§ˆ ê¸°ì¤€ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    analyzer = UniversalLineAnalyzer()
    analyzer.run_universal_analysis()