# .github/scripts/universal_line_analyzer.py
import os
import openai
from github import Github
from typing import List, Dict
from universal_code_analyzer import UniversalCodeAnalyzer

class UniversalLineAnalyzer:
    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])
        self.github_client = Github(os.environ['GITHUB_TOKEN'])
        self.repo_name = os.environ['REPO_NAME']
        self.pr_number = int(os.environ['PR_NUMBER'])

        self.repo = self.github_client.get_repo(self.repo_name)
        self.pr = self.repo.get_pull(self.pr_number)

        # ë²”ìš© ì½”ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.universal_analyzer = UniversalCodeAnalyzer(self.repo, self.pr)

    def read_conventions(self):
        """READMEì—ì„œ ì»¨ë²¤ì…˜ ì •ë³´ ì½ê¸°"""
        try:
            readme = self.repo.get_contents("README.md")
            readme_content = readme.decoded_content.decode('utf-8')

            import re
            convention_match = re.search(
                r'## AI ë¦¬ë·° ê°€ì´ë“œë¼ì¸.*?(?=##|$)',
                readme_content,
                re.DOTALL | re.IGNORECASE
            )

            if convention_match:
                return convention_match.group(0)
            else:
                return "ì»¨ë²¤ì…˜ ê°€ì´ë“œë¼ì¸ì´ READMEì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f"README ì½ê¸° ì‹¤íŒ¨: {e}")
            return "ì»¨ë²¤ì…˜ ì •ë³´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def analyze_file_for_issues(self, file_path: str, file_content: str, patch: str, conventions: str) -> List[Dict]:
        """íŒŒì¼ ë¶„ì„ (ì •ì  ë¶„ì„ + AI ë¶„ì„)"""

        language = self.universal_analyzer.detect_language(file_path)
        if not language:
            print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
            return []

        all_issues = []

        # 1. ì •ì  ë¶„ì„ (ì–¸ì–´ë³„ ë¦°í„°)
        static_issues = self.universal_analyzer.analyze_file(file_path, file_content)
        all_issues.extend(static_issues)

        # 2. AI ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„
        ai_issues = self.analyze_with_ai_advanced(file_path, file_content, patch, conventions, language)
        all_issues.extend(ai_issues)

        return all_issues

    def analyze_with_ai_advanced(self, file_path: str, file_content: str, patch: str, conventions: str, language: str) -> List[Dict]:
        """AI ê¸°ë°˜ ê³ ê¸‰ ì½”ë“œ í’ˆì§ˆ ë¶„ì„"""

        # ì–¸ì–´ë³„ íŠ¹í™” ë¶„ì„ í¬ì¸íŠ¸
        language_specific_points = {
            'kotlin': [
                "Android ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (Handler, Listener ë“±)",
                "ì½”ë£¨í‹´ ìŠ¤ì½”í”„ ê´€ë¦¬",
                "Room ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”",
                "Compose ë¦¬ì»´í¬ì§€ì…˜ ìµœì í™”"
            ],
            'swift': [
                "iOS ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (ê°•í•œ ìˆœí™˜ ì°¸ì¡°)",
                "DispatchQueue ì‚¬ìš© ìµœì í™”",
                "Core Data ì„±ëŠ¥ ë¬¸ì œ",
                "UIKit ìƒëª…ì£¼ê¸° ê´€ë¦¬"
            ],
            'javascript': [
                "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ, í´ë¡œì €)",
                "ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”",
                "DOM ì¡°ì‘ ì„±ëŠ¥",
                "ë²ˆë“¤ í¬ê¸° ìµœì í™”"
            ]
        }

        specific_points = language_specific_points.get(language, [])

        analysis_prompt = f"""
ë‹¹ì‹ ì€ {language} ì½”ë“œ ë¦¬ë·° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •ì  ë¶„ì„ ë„êµ¬ë¡œëŠ” ì°¾ê¸° ì–´ë ¤ìš´ ê³ ê¸‰ ë¬¸ì œì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**íŒŒì¼:** {file_path}
**ì–¸ì–´:** {language}
**íŒ€ ì»¨ë²¤ì…˜:** {conventions}

**{language} íŠ¹í™” ë¶„ì„ í¬ì¸íŠ¸:**
{chr(10).join(f'- {point}' for point in specific_points)}

**íŒŒì¼ ë‚´ìš© (ì¼ë¶€):**
```{language}
{file_content[:2000]}
```

**ë³€ê²½ì‚¬í•­:**
```diff
{patch[:1500]}
```

**ë¶„ì„ ëŒ€ìƒ:**

**P2 (ì¤‘ê°„ ìš°ì„ ìˆœìœ„):**
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜
- ì„±ëŠ¥ ì´ìŠˆ (O(nÂ²) ì•Œê³ ë¦¬ì¦˜, ë¶ˆí•„ìš”í•œ ì—°ì‚°)
- ì•ˆí‹°íŒ¨í„´ (God Object, ê°•í•œ ê²°í•©)
- ë™ì‹œì„±/ë¹„ë™ê¸° ì²˜ë¦¬ ë¬¸ì œ
- ë³´ì•ˆ ì·¨ì•½ì 

**P3 (ë‚®ì€ ìš°ì„ ìˆœìœ„):**
- ë³µì¡í•œ ë¡œì§ (ìˆœí™˜ ë³µì¡ë„ ë†’ìŒ)
- ì½”ë“œ ì¤‘ë³µ
- ë§¤ì§ ë„˜ë²„/ìŠ¤íŠ¸ë§
- ê³¼ë„í•œ ë§¤ê°œë³€ìˆ˜
- ë„¤ì´ë° ê°œì„  ì—¬ì§€

**ì‘ë‹µ í˜•ì‹:**
```json
[
  {{
    "line": ì¤„ë²ˆí˜¸,
    "priority": "P2"|"P3",
    "category": "ë©”ëª¨ë¦¬|ì„±ëŠ¥|ì•ˆí‹°íŒ¨í„´|ë™ì‹œì„±|ë³´ì•ˆ|ë³µì¡ë„|ì¤‘ë³µ|ë„¤ì´ë°",
    "message": "êµ¬ì²´ì ì¸ ë¬¸ì œì™€ {language} íŠ¹í™” ê°œì„ ë°©ì•ˆ",
    "suggestion": "ê°œì„ ëœ ì½”ë“œ ì˜ˆì‹œ"
  }}
]
```

ë³€ê²½ëœ ë¶€ë¶„ë§Œ ë¶„ì„í•˜ê³ , ì‹¤ì œ ë¬¸ì œê°€ ìˆì„ ë•Œë§Œ ë³´ê³ í•´ì£¼ì„¸ìš”.
"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"{language} ì „ë¬¸ ì½”ë“œ ë¦¬ë·°ì–´ë¡œì„œ ì •ì  ë¶„ì„ ë„êµ¬ê°€ ë†“ì¹˜ëŠ” ê³ ê¸‰ ë¬¸ì œë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=1200,
                temperature=0.1
            )

            response_text = response.choices[0].message.content.strip()

            import json
            try:
                issues = json.loads(response_text)
                return issues if isinstance(issues, list) else []
            except json.JSONDecodeError:
                print(f"AI ë¶„ì„ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:200]}")
                return []

        except Exception as e:
            print(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def create_review_comments(self, all_issues: Dict[str, List[Dict]]):
        """GitHub Review APIë¡œ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ ìƒì„±"""

        if not any(all_issues.values()):
            print("ë°œê²¬ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        comments = []
        linter_counts = {}  # ë¦°í„°ë³„ ì´ìŠˆ ê°œìˆ˜
        ai_count = 0

        for file_path, issues in all_issues.items():
            language = self.universal_analyzer.detect_language(file_path)

            for issue in issues:
                # ì´ìŠˆ ì¶œì²˜ êµ¬ë¶„
                category = issue.get('category', 'unknown')

                if category in ['ktlint', 'swiftlint', 'eslint']:
                    linter_counts[category] = linter_counts.get(category, 0) + 1
                    source_emoji = 'ğŸ”§'
                    source_text = category
                else:
                    ai_count += 1
                    source_emoji = 'ğŸ¤–'
                    source_text = 'AI ë¶„ì„'

                # ìš°ì„ ìˆœìœ„ë³„ ì´ëª¨ì§€
                priority_emoji = {'P2': 'ğŸŸ¡', 'P3': 'ğŸ”µ'}

                comment_body = f"{priority_emoji.get(issue['priority'], 'ğŸ“')} **[{issue['priority']}]** {source_emoji} **{source_text}**\n\n"
                comment_body += f"**{issue['category']}**: {issue['message']}\n"

                if issue.get('suggestion'):
                    comment_body += f"\n**ğŸ’¡ ê°œì„  ì œì•ˆ:**\n```{language}\n{issue['suggestion']}\n```"

                comments.append({
                    'path': file_path,
                    'line': issue['line'],
                    'body': comment_body
                })

        # GitHub Review ìƒì„±
        try:
            review_body = f"ğŸ¤– **ë²”ìš© ì½”ë“œ í’ˆì§ˆ ìë™ ê²€ìˆ˜ ê²°ê³¼**\n\n"

            # ë¦°í„°ë³„ ì´ìŠˆ ê°œìˆ˜ í‘œì‹œ
            for linter, count in linter_counts.items():
                review_body += f"ğŸ”§ **{linter}**: {count}ê°œ ì´ìŠˆ\n"

            if ai_count > 0:
                review_body += f"ğŸ¤– **AI ê³ ê¸‰ ë¶„ì„**: {ai_count}ê°œ ì´ìŠˆ\n"

            review_body += f"\n**ì§€ì› ì–¸ì–´:** {', '.join(self.universal_analyzer.linters.keys())}\n"
            review_body += "ê²€í†  í›„ í•„ìš”ì‹œ ìˆ˜ì •í•´ì£¼ì„¸ìš”. ì •ì  ë¶„ì„ ì´ìŠˆëŠ” IDEì—ì„œ ìë™ ìˆ˜ì • ê°€ëŠ¥í•©ë‹ˆë‹¤."

            review = self.pr.create_review(
                body=review_body,
                event="COMMENT",
                comments=comments
            )

            total_static = sum(linter_counts.values())
            print(f"âœ… ì´ {len(comments)}ê°œ ì½”ë©˜íŠ¸ (ì •ì ë¶„ì„: {total_static}, AI: {ai_count})ê°€ í¬í•¨ëœ ë¦¬ë·°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {review.html_url}")

        except Exception as e