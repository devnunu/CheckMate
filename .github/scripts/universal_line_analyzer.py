# .github/scripts/universal_line_analyzer.py
import os
import re
import openai
from github import Github
from typing import List, Dict
from universal_code_analyzer import UniversalCodeAnalyzer

class UniversalLineAnalyzer:
    def __init__(self):
        self.openai_client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])
        self.github_client = Github(os.environ['GITHUB_TOKEN'])
        self.repo_name = os.environ['REPO_NAME']
        self.pr_number = int(os.environ['PR_NUMBER'])

        self.repo = self.github_client.get_repo(self.repo_name)
        self.pr = self.repo.get_pull(self.pr_number)

        # ë²”ìš© ì½”ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.universal_analyzer = UniversalCodeAnalyzer(self.repo, self.pr)

    def read_conventions(self):
        """READMEì—ì„œ ì»¨ë²¤ì…˜ ì •ë³´ ì½ê¸°"""
        try:
            readme = self.repo.get_contents("README.md")
            readme_content = readme.decoded_content.decode('utf-8')

            convention_match = re.search(
                r'## AI ë¦¬ë·° ê°€ì´ë“œë¼ì¸.*?(?=##|$)',
                readme_content,
                re.DOTALL | re.IGNORECASE
            )

            if convention_match:
                return convention_match.group(0)
            else:
                return "ì»¨ë²¤ì…˜ ê°€ì´ë“œë¼ì¸ì´ READMEì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f"README ì½ê¸° ì‹¤íŒ¨: {e}")
            return "ì»¨ë²¤ì…˜ ì •ë³´ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def analyze_file_for_issues(self, file_path: str, file_content: str, patch: str, conventions: str) -> List[Dict]:
        """íŒŒì¼ ë¶„ì„ (AI ê¸°ë°˜ ë¦°íŠ¸ ë¶„ì„ + AI ê³ ê¸‰ ë¶„ì„)"""

        language = self.universal_analyzer.detect_language(file_path)
        if not language:
            print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
            return []

        all_issues = []

        # 1. AI ê¸°ë°˜ ë¦°íŠ¸ ë¶„ì„ (ì„¤ì • íŒŒì¼ ê¸°ë°˜)
        lint_issues = self.universal_analyzer.analyze_file(file_path, file_content)
        all_issues.extend(lint_issues)

        # 2. AI ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜, ì•ˆí‹°íŒ¨í„´ ë“±)
        ai_issues = self.analyze_with_ai_advanced(file_path, file_content, patch, conventions, language)
        all_issues.extend(ai_issues)

        return all_issues

    def analyze_with_ai_advanced(self, file_path: str, file_content: str, patch: str, conventions: str, language: str) -> List[Dict]:
        """AI ê¸°ë°˜ ê³ ê¸‰ ì½”ë“œ í’ˆì§ˆ ë¶„ì„"""

        # ì–¸ì–´ë³„ íŠ¹í™” ë¶„ì„ í¬ì¸íŠ¸
        language_specific_points = {
            'kotlin': [
                "Android ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (Handler, Listener ë“±)",
                "ì½”ë£¨í‹´ ìŠ¤ì½”í”„ ê´€ë¦¬",
                "Room ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”",
                "Compose ë¦¬ì»´í¬ì§€ì…˜ ìµœì í™”"
            ],
            'swift': [
                "iOS ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (ê°•í•œ ìˆœí™˜ ì°¸ì¡°)",
                "DispatchQueue ì‚¬ìš© ìµœì í™”",
                "Core Data ì„±ëŠ¥ ë¬¸ì œ",
                "UIKit ìƒëª…ì£¼ê¸° ê´€ë¦¬"
            ],
            'javascript': [
                "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ, í´ë¡œì €)",
                "ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”",
                "DOM ì¡°ì‘ ì„±ëŠ¥",
                "ë²ˆë“¤ í¬ê¸° ìµœì í™”"
            ]
        }

        specific_points = language_specific_points.get(language, [])

        analysis_prompt = f"""
ë‹¹ì‹ ì€ {language} ì½”ë“œ ë¦¬ë·° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •ì  ë¶„ì„ ë„êµ¬ë¡œëŠ” ì°¾ê¸° ì–´ë ¤ìš´ ê³ ê¸‰ ë¬¸ì œì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**íŒŒì¼:** {file_path}
**ì–¸ì–´:** {language}
**íŒ€ ì»¨ë²¤ì…˜:** {conventions}

**{language} íŠ¹í™” ë¶„ì„ í¬ì¸íŠ¸:**
{chr(10).join(f'- {point}' for point in specific_points)}

**íŒŒì¼ ë‚´ìš© (ì¼ë¶€):**
```{language}
{file_content[:2000]}
```

**ë³€ê²½ì‚¬í•­:**
```diff
{patch[:1500]}
```

**ë¶„ì„ ëŒ€ìƒ:**

**P2 (ì¤‘ê°„ ìš°ì„ ìˆœìœ„):**
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜
- ì„±ëŠ¥ ì´ìŠˆ (O(nÂ²) ì•Œê³ ë¦¬ì¦˜, ë¶ˆí•„ìš”í•œ ì—°ì‚°)
- ì•ˆí‹°íŒ¨í„´ (God Object, ê°•í•œ ê²°í•©)
- ë™ì‹œì„±/ë¹„ë™ê¸° ì²˜ë¦¬ ë¬¸ì œ
- ë³´ì•ˆ ì·¨ì•½ì 

**P3 (ë‚®ì€ ìš°ì„ ìˆœìœ„):**
- ë³µì¡í•œ ë¡œì§ (ìˆœí™˜ ë³µì¡ë„ ë†’ìŒ)
- ì½”ë“œ ì¤‘ë³µ
- ë§¤ì§ ë„˜ë²„/ìŠ¤íŠ¸ë§
- ê³¼ë„í•œ ë§¤ê°œë³€ìˆ˜
- ë„¤ì´ë° ê°œì„  ì—¬ì§€

**ì‘ë‹µ í˜•ì‹:**
```json
[
  {{
    "line": ì¤„ë²ˆí˜¸,
    "priority": "P2"|"P3",
    "category": "ë©”ëª¨ë¦¬|ì„±ëŠ¥|ì•ˆí‹°íŒ¨í„´|ë™ì‹œì„±|ë³´ì•ˆ|ë³µì¡ë„|ì¤‘ë³µ|ë„¤ì´ë°",
    "message": "êµ¬ì²´ì ì¸ ë¬¸ì œì™€ {language} íŠ¹í™” ê°œì„ ë°©ì•ˆ",
    "suggestion": "ê°œì„ ëœ ì½”ë“œ ì˜ˆì‹œ"
  }}
]
```

ë³€ê²½ëœ ë¶€ë¶„ë§Œ ë¶„ì„í•˜ê³ , ì‹¤ì œ ë¬¸ì œê°€ ìˆì„ ë•Œë§Œ ë³´ê³ í•´ì£¼ì„¸ìš”.
"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"{language} ì „ë¬¸ ì½”ë“œ ë¦¬ë·°ì–´ë¡œì„œ ì •ì  ë¶„ì„ ë„êµ¬ê°€ ë†“ì¹˜ëŠ” ê³ ê¸‰ ë¬¸ì œë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=1200,
                temperature=0.1
            )

            response_text = response.choices[0].message.content.strip()

            import json
            try:
                issues = json.loads(response_text)
                return issues if isinstance(issues, list) else []
            except json.JSONDecodeError:
                print(f"AI ë¶„ì„ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:200]}")
                return []

        except Exception as e:
            print(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def get_existing_review_comments(self):
        """ê¸°ì¡´ AI ë¦¬ë·° ì½”ë©˜íŠ¸ ë¶„ì„"""
        existing_comments = {}

        try:
            # PRì˜ ëª¨ë“  ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
            reviews = self.pr.get_reviews()

            for review in reviews:
                # github-actions botì´ ì‘ì„±í•œ ë¦¬ë·°ë§Œ í™•ì¸
                if review.user.login == 'github-actions[bot]':
                    # ë¦¬ë·°ì˜ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    review_comments = self.pr.get_review_comments()

                    for comment in review_comments:
                        if comment.user.login == 'github-actions[bot]':
                            # íŒŒì¼ ê²½ë¡œì™€ ë¼ì¸ ë²ˆí˜¸ë¥¼ í‚¤ë¡œ ì‚¬ìš©
                            key = f"{comment.path}:{comment.line}"

                            # ì½”ë©˜íŠ¸ ë‚´ìš©ì—ì„œ í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„, ì¹´í…Œê³ ë¦¬, ë©”ì‹œì§€)
                            comment_info = self.extract_comment_info(comment.body)
                            existing_comments[key] = comment_info

            return existing_comments

        except Exception as e:
            print(f"ê¸°ì¡´ ì½”ë©˜íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def extract_comment_info(self, comment_body: str):
        """ì½”ë©˜íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            import re

            # ìš°ì„ ìˆœìœ„ ì¶”ì¶œ [P2] ë˜ëŠ” [P3]
            priority_match = re.search(r'\[P([23])\]', comment_body)
            priority = f"P{priority_match.group(1)}" if priority_match else "P3"

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ **ì¹´í…Œê³ ë¦¬**: ë©”ì‹œì§€
            category_match = re.search(r'\*\*([^*]+)\*\*:\s*([^\n]+)', comment_body)
            category = category_match.group(1).strip() if category_match else "unknown"
            message = category_match.group(2).strip() if category_match else ""

            return {
                'priority': priority,
                'category': category,
                'message': message,
                'full_body': comment_body
            }

        except Exception:
            return {
                'priority': 'P3',
                'category': 'unknown',
                'message': '',
                'full_body': comment_body
            }

    def is_duplicate_comment(self, new_issue: Dict, existing_comment: Dict) -> bool:
        """ìƒˆ ì´ìŠˆê°€ ê¸°ì¡´ ì½”ë©˜íŠ¸ì™€ ì¤‘ë³µì¸ì§€ í™•ì¸"""

        # ìš°ì„ ìˆœìœ„ì™€ ì¹´í…Œê³ ë¦¬ê°€ ê°™ì€ì§€ í™•ì¸
        if (new_issue.get('priority') == existing_comment.get('priority') and
            new_issue.get('category') == existing_comment.get('category')):

            # ë©”ì‹œì§€ ìœ ì‚¬ë„ í™•ì¸ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
            new_message = new_issue.get('message', '').lower()
            existing_message = existing_comment.get('message', '').lower()

            # í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            key_phrases = [
                'ë“¤ì—¬ì“°ê¸°', '4ì¹¸ ìŠ¤í˜ì´ìŠ¤', 'camelcase', 'í•¨ìˆ˜ëª…', 'ë³€ìˆ˜ëª…',
                'ë©”ëª¨ë¦¬ ëˆ„ìˆ˜', 'ì„±ëŠ¥', 'ì•ˆí‹°íŒ¨í„´', 'ë³µì¡ë„', 'ë„¤ì´ë°'
            ]

            for phrase in key_phrases:
                if phrase in new_message and phrase in existing_message:
                    return True

            # ë©”ì‹œì§€ê°€ 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
            similarity = self.calculate_message_similarity(new_message, existing_message)
            if similarity > 0.7:
                return True

        return False

    def calculate_message_similarity(self, message1: str, message2: str) -> float:
        """ë‘ ë©”ì‹œì§€ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë‹¨ì–´ ê¸°ë°˜)"""
        if not message1 or not message2:
            return 0.0

        words1 = set(message1.split())
        words2 = set(message2.split())

        if not words1 or not words2:
            return 0.0

        intersection = words1.intersection(words2)
        union = words1.union(words2)

        return len(intersection) / len(union)

    def filter_duplicate_issues(self, all_issues: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """ì¤‘ë³µ ì´ìŠˆ í•„í„°ë§"""

        print("ğŸ” ê¸°ì¡´ ì½”ë©˜íŠ¸ì™€ ì¤‘ë³µ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...")

        # ê¸°ì¡´ ì½”ë©˜íŠ¸ ê°€ì ¸ì˜¤ê¸°
        existing_comments = self.get_existing_review_comments()

        if not existing_comments:
            print("  âœ… ê¸°ì¡´ AI ì½”ë©˜íŠ¸ê°€ ì—†ìŒ - ëª¨ë“  ì´ìŠˆ ë¶„ì„")
            return all_issues

        filtered_issues = {}
        total_issues = 0
        duplicate_count = 0

        for file_path, issues in all_issues.items():
            filtered_file_issues = []

            for issue in issues:
                total_issues += 1
                line = issue['line']
                comment_key = f"{file_path}:{line}"

                # í•´ë‹¹ ë¼ì¸ì— ê¸°ì¡´ ì½”ë©˜íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                if comment_key in existing_comments:
                    existing_comment = existing_comments[comment_key]

                    # ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
                    if self.is_duplicate_comment(issue, existing_comment):
                        duplicate_count += 1
                        print(f"  â­ï¸ ì¤‘ë³µ ê±´ë„ˆë›°ê¸°: {file_path}:{line} - {issue['category']}")
                        continue
                    else:
                        print(f"  ğŸ”„ ë‹¤ë¥¸ ì´ìŠˆ ê°ì§€: {file_path}:{line} - {issue['category']}")

                filtered_file_issues.append(issue)

            if filtered_file_issues:
                filtered_issues[file_path] = filtered_file_issues

        print(f"ğŸ“Š ì¤‘ë³µ í•„í„°ë§ ê²°ê³¼: ì „ì²´ {total_issues}ê°œ ì¤‘ {duplicate_count}ê°œ ì¤‘ë³µ ì œê±°")
        return filtered_issues

    def create_review_comments(self, all_issues: Dict[str, List[Dict]]):
        """GitHub Review APIë¡œ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ ìƒì„± (ì¤‘ë³µ ë°©ì§€ í¬í•¨)"""

        if not any(all_issues.values()):
            print("ë°œê²¬ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì¤‘ë³µ ì´ìŠˆ í•„í„°ë§
        filtered_issues = self.filter_duplicate_issues(all_issues)

        if not any(filtered_issues.values()):
            print("âœ… ëª¨ë“  ì´ìŠˆê°€ ê¸°ì¡´ ì½”ë©˜íŠ¸ì™€ ì¤‘ë³µë©ë‹ˆë‹¤. ìƒˆë¡œìš´ ì½”ë©˜íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        comments = []
        linter_counts = {}  # ë¦°í„°ë³„ ì´ìŠˆ ê°œìˆ˜
        advanced_count = 0

        for file_path, issues in filtered_issues.items():
            language = self.universal_analyzer.detect_language(file_path)

            for issue in issues:
                # ì´ìŠˆ ì¶œì²˜ êµ¬ë¶„
                category = issue.get('category', 'unknown')

                if category in ['kotlinlint', 'swiftlint', 'eslint']:
                    linter_counts[category] = linter_counts.get(category, 0) + 1
                    source_emoji = 'ğŸ¤–'
                    source_text = f'AI {category}'
                else:
                    advanced_count += 1
                    source_emoji = 'ğŸ§ '
                    source_text = 'AI ê³ ê¸‰ë¶„ì„'

                # ìš°ì„ ìˆœìœ„ë³„ ì´ëª¨ì§€
                priority_emoji = {'P2': 'ğŸŸ¡', 'P3': 'ğŸ”µ'}

                comment_body = f"{priority_emoji.get(issue['priority'], 'ğŸ“')} **[{issue['priority']}] {source_text}**\n\n"
                comment_body += f"**{issue['category']}**: {issue['message']}\n"

                if issue.get('suggestion'):
                    comment_body += f"\n**ğŸ’¡ ê°œì„  ì œì•ˆ:**\n```{language}\n{issue['suggestion']}\n```"

                # GitHub Review API ì½”ë©˜íŠ¸ í˜•ì‹
                comments.append({
                    'path': file_path,
                    'body': comment_body,
                    'line': issue['line'],
                    'side': 'RIGHT'  # ë³€ê²½ëœ ì½”ë“œ ë¼ì¸ì— ì½”ë©˜íŠ¸ (RIGHT = ìƒˆ ë²„ì „, LEFT = ì´ì „ ë²„ì „)
                })

        # GitHub Review ìƒì„± (ìš”ì•½ ì½”ë©˜íŠ¸ ì—†ì´ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ë§Œ)
        try:
            # Review ìƒì„± (body ì—†ì´ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ë§Œ)
            review = self.pr.create_review(
                event="COMMENT",
                comments=comments
            )

            total_lint = sum(linter_counts.values())
            print(f"âœ… ì´ {len(comments)}ê°œ ìƒˆë¡œìš´ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ (AI ë¦°íŠ¸: {total_lint}, ê³ ê¸‰ë¶„ì„: {advanced_count})ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {review.html_url}")

        except Exception as e:
            print(f"âŒ ë¼ì¸ë³„ ë¦¬ë·° ìƒì„± ì‹¤íŒ¨: {e}")
            print("Diff ê¸°ë°˜ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ë¡œ ì¬ì‹œë„...")
            self.create_diff_based_comments(filtered_issues)

    def create_diff_based_comments(self, filtered_issues: Dict[str, List[Dict]]):
        """Diff ê¸°ë°˜ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ ìƒì„± (ëŒ€ì²´ ë°©ë²•)"""

        comments = []

        for file_path, issues in filtered_issues.items():
            # PRì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ diff ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            pr_file = None
            for file in self.pr.get_files():
                if file.filename == file_path:
                    pr_file = file
                    break

            if not pr_file or not pr_file.patch:
                continue

            # diffì—ì„œ ë³€ê²½ëœ ë¼ì¸ ë²ˆí˜¸ ë§¤í•‘
            diff_line_mapping = self.parse_diff_line_mapping(pr_file.patch)

            language = self.universal_analyzer.detect_language(file_path)

            for issue in issues:
                file_line = issue['line']

                # ì‹¤ì œ íŒŒì¼ ë¼ì¸ì„ diff ë¼ì¸ìœ¼ë¡œ ë³€í™˜
                diff_line = self.convert_file_line_to_diff_line(file_line, diff_line_mapping)

                if diff_line is None:
                    continue  # ë³€ê²½ë˜ì§€ ì•Šì€ ë¼ì¸ì€ ì½”ë©˜íŠ¸ ë¶ˆê°€

                category = issue.get('category', 'unknown')
                if category in ['ktlint', 'swiftlint', 'eslint']:
                    source_emoji = 'ğŸ”§'
                    source_text = category
                else:
                    source_emoji = 'ğŸ¤–'
                    source_text = 'AI ë¶„ì„'

                priority_emoji = {'P2': 'ğŸŸ¡', 'P3': 'ğŸ”µ'}

                comment_body = f"{priority_emoji.get(issue['priority'], 'ğŸ“')} **[{issue['priority']}] {source_text}**\n\n"
                comment_body += f"**{issue['category']}**: {issue['message']}\n"

                if issue.get('suggestion'):
                    comment_body += f"\n**ğŸ’¡ ê°œì„  ì œì•ˆ:**\n```{language}\n{issue['suggestion']}\n```"

                comments.append({
                    'path': file_path,
                    'body': comment_body,
                    'position': diff_line  # diff ë‚´ì—ì„œì˜ ìœ„ì¹˜
                })

        # Review ìƒì„± (ìš”ì•½ ì½”ë©˜íŠ¸ ì—†ì´)
        try:
            review = self.pr.create_review(
                event="COMMENT",
                comments=comments
            )

            print(f"âœ… {len(comments)}ê°œ diff ê¸°ë°˜ ìƒˆë¡œìš´ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {review.html_url}")

        except Exception as e:
            print(f"âŒ Diff ê¸°ë°˜ ì½”ë©˜íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨: {e}")
            self.create_fallback_comment(filtered_issues)

    def parse_diff_line_mapping(self, patch: str) -> Dict[int, int]:
        """diff patchì—ì„œ íŒŒì¼ ë¼ì¸ ë²ˆí˜¸ â†’ diff ìœ„ì¹˜ ë§¤í•‘ ìƒì„±"""
        mapping = {}
        lines = patch.split('\n')

        current_new_line = 0
        diff_position = 0

        for line in lines:
            if line.startswith('@@'):
                # @@ -old_start,old_count +new_start,new_count @@ í˜•ì‹ íŒŒì‹±
                import re
                match = re.search(r'\+(\d+)', line)
                if match:
                    current_new_line = int(match.group(1)) - 1
            elif line.startswith('+'):
                current_new_line += 1
                mapping[current_new_line] = diff_position
            elif line.startswith(' '):
                current_new_line += 1
                mapping[current_new_line] = diff_position
            # '-'ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ì€ current_new_line ì¦ê°€í•˜ì§€ ì•ŠìŒ

            diff_position += 1

        return mapping

    def convert_file_line_to_diff_line(self, file_line: int, mapping: Dict[int, int]) -> int:
        """íŒŒì¼ ë¼ì¸ ë²ˆí˜¸ë¥¼ diff ìœ„ì¹˜ë¡œ ë³€í™˜"""
        return mapping.get(file_line)

    def create_fallback_comment(self, filtered_issues: Dict[str, List[Dict]]):
        """Review API ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì½”ë©˜íŠ¸ë¡œ ëŒ€ì²´"""
        comment_body = "ğŸ¤– **ë²”ìš© ì½”ë“œ í’ˆì§ˆ ê²€ìˆ˜ ê²°ê³¼**\n\n"

        for file_path, issues in filtered_issues.items():
            if issues:
                language = self.universal_analyzer.detect_language(file_path)
                comment_body += f"\n### ğŸ“ {file_path} ({language})\n"

                for issue in issues:
                    category = issue.get('category', 'unknown')
                    if category in ['ktlint', 'swiftlint', 'eslint']:
                        source_emoji = 'ğŸ”§'
                    else:
                        source_emoji = 'ğŸ¤–'

                    priority_emoji = {'P2': 'ğŸŸ¡', 'P3': 'ğŸ”µ'}
                    comment_body += f"- **Line {issue['line']}** {priority_emoji.get(issue['priority'], 'ğŸ“')} [{issue['priority']}] {source_emoji} {issue['category']}: {issue['message']}\n"

        try:
            self.pr.create_issue_comment(comment_body)
            print("âœ… ëŒ€ì²´ ì½”ë©˜íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ëŒ€ì²´ ì½”ë©˜íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨: {e}")

    def run_universal_analysis(self):
        """ë²”ìš© ë¶„ì„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ” ë²”ìš© ì½”ë“œ í’ˆì§ˆ ê²€ìˆ˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # ë¶„ì„ ì„¤ì • ìš”ì•½
        analysis_summary = self.universal_analyzer.get_analysis_summary()
        print(f"ğŸ“‹ {analysis_summary}")

        # ì»¨ë²¤ì…˜ ì •ë³´ ì½ê¸°
        conventions = self.read_conventions()

        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        supported_extensions = self.universal_analyzer.get_supported_extensions()

        # PRì˜ ë³€ê²½ëœ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
        files = self.pr.get_files()
        all_issues = {}
        analyzed_count = 0
        skipped_count = 0

        for file in files:
            # ì‚­ì œëœ íŒŒì¼ ê±´ë„ˆë›°ê¸°
            if file.status == 'removed':
                continue

            # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì í™•ì¸
            is_supported = any(file.filename.endswith(ext) for ext in supported_extensions)
            if not is_supported:
                skipped_count += 1
                continue

            print(f"ğŸ“ ë¶„ì„ ì¤‘: {file.filename}")
            analyzed_count += 1

            try:
                # í˜„ì¬ íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                content = self.repo.get_contents(file.filename, ref=self.pr.head.sha)
                file_content = content.decoded_content.decode('utf-8')

                # íŒŒì¼ë³„ ì´ìŠˆ ë¶„ì„
                issues = self.analyze_file_for_issues(
                    file.filename,
                    file_content,
                    file.patch or "",
                    conventions
                )

                if issues:
                    all_issues[file.filename] = issues

                    # ì´ìŠˆ ë¶„ë¥˜ë³„ ê°œìˆ˜ ê³„ì‚°
                    lint_issues = [i for i in issues if i.get('category') in ['kotlinlint', 'swiftlint', 'eslint']]
                    advanced_issues = [i for i in issues if i.get('category') not in ['kotlinlint', 'swiftlint', 'eslint']]

                    print(f"  âš ï¸ ì´ {len(issues)}ê°œ ì´ìŠˆ (ë¦°íŠ¸: {len(lint_issues)}, ê³ ê¸‰ë¶„ì„: {len(advanced_issues)})")
                else:
                    print(f"  âœ… ì´ìŠˆ ì—†ìŒ")

            except Exception as e:
                print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue

        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ë¶„ì„ ì™„ë£Œ: {analyzed_count}ê°œ íŒŒì¼ ë¶„ì„, {skipped_count}ê°œ íŒŒì¼ ê±´ë„ˆë›°ê¸°")

        # ë¦¬ë·° ì½”ë©˜íŠ¸ ìƒì„±
        if all_issues:
            # ì „ì²´ ì´ìŠˆ í†µê³„
            total_static = 0
            total_ai = 0
            linter_stats = {}

            for issues in all_issues.values():
                for issue in issues:
                    category = issue.get('category', 'unknown')
                    if category in ['kotlinlint', 'swiftlint', 'eslint']:
                        total_lint += 1
                        linter_stats[category] = linter_stats.get(category, 0) + 1
                    else:
                        total_advanced += 1

            print(f"ğŸ“ˆ ê²€ìˆ˜ ì™„ë£Œ:")
            for linter, count in linter_stats.items():
                print(f"  ğŸ”§ {linter}: {count}ê°œ")
            if total_ai > 0:
                print(f"  ğŸ¤– AI ë¶„ì„: {total_ai}ê°œ")

            self.create_review_comments(all_issues)
        else:
            print("âœ… ëª¨ë“  ë¶„ì„ ëŒ€ìƒ íŒŒì¼ì´ í’ˆì§ˆ ê¸°ì¤€ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    analyzer = UniversalLineAnalyzer()
    analyzer.run_universal_analysis()